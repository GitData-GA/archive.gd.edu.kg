<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>GitData Archive - Data Efficient Dense Cross-Lingual Information Retrieval</title>
    <link rel="icon" type="image/x-icon" href="https://static.gd.edu.kg/images/logo.svg">
    <link rel="apple-touch-icon" href="https://static.gd.edu.kg/images/logo.svg">
    <link rel="canonical" href="https://archive.gd.edu.kg/abs/20241230074419/" />
    <meta name="predecessor" content="null">
    <meta name="successor" content="null">
    <meta name="submissionID" content="20241230074419">
    <meta name="keywords" content="LLMs, Cross-Lingual Information Retrieval (CIR), Translation">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ELSZ3SFQ2D"></script>
    <script>
      function gtag() {
        dataLayer.push(arguments)
      }
      window.dataLayer = window.dataLayer || [], gtag("js", new Date), gtag("config", "G-ELSZ3SFQ2D")
    </script>
    <meta name="citation_title" content="Data Efficient Dense Cross-Lingual Information Retrieval">
    <meta name="citation_author" content="Chen, Luc">
<meta name="citation_author" content="He, Yinan">
<meta name="citation_author" content="Nguyen, Alayna">
    <meta name="citation_publication_date" content="2024/12/30">
    <meta name="citation_journal_title" content="GitData Archive">
    <meta name="citation_volume" content="2024">
    <meta name="citation_gd_archive_id" content="20241230074419">
    <meta name="citation_pdf_url" content="https://archive.gd.edu.kg/pdf/20241230074419/v1.pdf">
    <meta name="citation_abstract" content="Cross-Lingual Information Retrieval (CIR) remains challenging due to limited annotated data and linguistic diversity, especially for low-resource languages. While dense retrieval models have significantly advanced retrieval performance, their reliance on large-scale training datasets hampers their effectiveness in multilingual settings. In this work, we propose two complementary strategies to improve data efficiency and robustness in CIR model fine- tuning. First, we introduce a paraphrase-based query augmentation pipeline leveraging large language models (LLMs) to enrich scarce training data, thereby promoting more robust and language-agnostic representations. Second, we present a weighted InfoNCE loss that emphasizes underrepresented languages, ensuring balanced optimization across heterogeneous linguistic inputs. Experiments on cross-lingual benchmark datasets demonstrate that our combined approaches yield substantial gains in retrieval quality, outperforming standard training protocols on small and imbalanced datasets. These results underscore the potential of targeted data augmentation and reweighted objectives to build more inclusive and effective CIR systems, even under resource constraints.">
    <meta itemprop="name" content="GitData Archive - Data Efficient Dense Cross-Lingual Information Retrieval">
    <meta name="description" content="Cross-Lingual Information Retrieval (CIR) remains challenging due to limited annotated data and linguistic diversity, especially for low-resource languages. While dense retrieval models have significantly advanced retrieval performance, their reliance on large-scale training datasets hampers their effectiveness in multilingual settings. In this work, we propose two complementary strategies to improve data efficiency and robustness in CIR model fine- tuning. First, we introduce a paraphrase-based query augmentation pipeline leveraging large language models (LLMs) to enrich scarce training data, thereby promoting more robust and language-agnostic representations. Second, we present a weighted InfoNCE loss that emphasizes underrepresented languages, ensuring balanced optimization across heterogeneous linguistic inputs. Experiments on cross-lingual benchmark datasets demonstrate that our combined approaches yield substantial gains in retrieval quality, outperforming standard training protocols on small and imbalanced datasets. These results underscore the potential of targeted data augmentation and reweighted objectives to build more inclusive and effective CIR systems, even under resource constraints.">
    <meta itemprop="image" content="https://archive.gd.edu.kg/assets/bg.jpg">
    <meta property="og:url" content="https://archive.gd.edu.kg/abs/20241230074419/">
    <meta property="og:type" content="website">
    <meta property="og:title" content="GitData Archive - Data Efficient Dense Cross-Lingual Information Retrieval">
    <meta property="og:description" content="Cross-Lingual Information Retrieval (CIR) remains challenging due to limited annotated data and linguistic diversity, especially for low-resource languages. While dense retrieval models have significantly advanced retrieval performance, their reliance on large-scale training datasets hampers their effectiveness in multilingual settings. In this work, we propose two complementary strategies to improve data efficiency and robustness in CIR model fine- tuning. First, we introduce a paraphrase-based query augmentation pipeline leveraging large language models (LLMs) to enrich scarce training data, thereby promoting more robust and language-agnostic representations. Second, we present a weighted InfoNCE loss that emphasizes underrepresented languages, ensuring balanced optimization across heterogeneous linguistic inputs. Experiments on cross-lingual benchmark datasets demonstrate that our combined approaches yield substantial gains in retrieval quality, outperforming standard training protocols on small and imbalanced datasets. These results underscore the potential of targeted data augmentation and reweighted objectives to build more inclusive and effective CIR systems, even under resource constraints.">
    <meta property="og:image" content="https://archive.gd.edu.kg/assets/bg.jpg">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="GitData Archive - Data Efficient Dense Cross-Lingual Information Retrieval">
    <meta name="twitter:description" content="Cross-Lingual Information Retrieval (CIR) remains challenging due to limited annotated data and linguistic diversity, especially for low-resource languages. While dense retrieval models have significantly advanced retrieval performance, their reliance on large-scale training datasets hampers their effectiveness in multilingual settings. In this work, we propose two complementary strategies to improve data efficiency and robustness in CIR model fine- tuning. First, we introduce a paraphrase-based query augmentation pipeline leveraging large language models (LLMs) to enrich scarce training data, thereby promoting more robust and language-agnostic representations. Second, we present a weighted InfoNCE loss that emphasizes underrepresented languages, ensuring balanced optimization across heterogeneous linguistic inputs. Experiments on cross-lingual benchmark datasets demonstrate that our combined approaches yield substantial gains in retrieval quality, outperforming standard training protocols on small and imbalanced datasets. These results underscore the potential of targeted data augmentation and reweighted objectives to build more inclusive and effective CIR systems, even under resource constraints.">
    <meta name="twitter:image" content="https://archive.gd.edu.kg/assets/bg.jpg">
    <link href="https://archive.gd.edu.kg/assets/app.css" rel="stylesheet">
    <link href="https://archive.gd.edu.kg/assets/bootstrap.min.css" rel="stylesheet">
    <link href="https://archive.gd.edu.kg/assets/custom.css" rel="stylesheet">
    <script src="https://archive.gd.edu.kg/assets/jquery.min.js" type="text/javascript"></script>
    <script src="https://archive.gd.edu.kg/assets/footerMenu.js" type="text/javascript"></script>
    <script src="https://archive.gd.edu.kg/assets/headMenu.js" type="text/javascript"></script>
    <script src="https://archive.gd.edu.kg/assets/pdf-lib.min.js" async></script>
    <script src="https://archive.gd.edu.kg/assets/pdfDownload.js" async></script>
    <script src="https://archive.gd.edu.kg/assets/showBibtex.js" async></script>
    <script src="https://mathjax.gd.edu.kg/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript" async></script>
    <script type="text/x-mathjax-config"> init_mathjax = function() {
                    if (window.MathJax) {
                    // MathJax loaded
                        MathJax.Hub.Config({
                            TeX: {
                                equationNumbers: {
                                autoNumber: "AMS",
                                useLabelIds: true
                                }
                            },
                            tex2jax: {
                                inlineMath: [ ['$','$'] ],
                                displayMath: [ ['$$','$$'] ],
                                processEscapes: true,
                                processEnvironments: true
                            },
                            displayAlign: 'center',
                            CommonHTML: {
                                linebreaks: {
                                automatic: true
                                }
                            }
                        });
                
                        MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
                    }
                }
                init_mathjax();
            </script>
  </head>
  <body>
    <nav role="navigation" class="mobile-menu">
      <div class="burger_container" id="burger">
        <div class="bar1"></div>
        <div class="bar2"></div>
        <div class="bar3"></div>
      </div>
      <div id="mobileMenu" class="mobile-back menu_off"></div>
      <div class="logo-mobile">
        <a href="https://archive.gd.edu.kg/">
          <img src="https://archive.gd.edu.kg/assets/logo.svg" width="150px" alt="logo">
        </a>
      </div>
      <script>
        $('#burger').click(function(e) {
          e.preventDefault();
          $(this).toggleClass('change');
          $('.mobile-menu').toggleClass('mobile-menu-open');
          $('.mobile-back').toggleClass('menu_on');
          $('.mobile-back').toggleClass('menu_off');
        })
      </script>
    </nav>
    <nav class="pre-navbar">
      <div class="logo-background" style="display: flex; justify-content: center">
        <a href="https://archive.gd.edu.kg/" style="display: flex; align-items: center">
          <img src="https://archive.gd.edu.kg/assets/logo.svg" width="170px" alt="logo">
        </a>
      </div>
      <div id="normalMenu" class="items-container"></div>
    </nav>
    <nav class="post-navbar"></nav>
    <div class="app" id="app">
      <div class="grid article-page d-flex">
        <div class="article-content col main-block">
          <div class="title">
            <b>
              <h1>Data Efficient Dense Cross-Lingual Information Retrieval</h1>
            </b>
          </div>
          <div class="notation">This article is a preprint and has not been peer-reviewed.</div>
          <br>
          <div class="doi" style="margin-bottom: 10px">
            <span>
              For citation:
            </span>
            <br>
            <a href="javascript:void(0);" id="show-bibtex" onclick="openBibTex()">Show BibTeX format</a>
            <div id="bibtexDiv" style="background-color: #C6E6F5; padding: 15px; position: relative; display: none;">
              <pre id="codeBlock">
@misc{gd_archive_20241230074419_v1,
  title={Data Efficient Dense Cross-Lingual Information Retrieval}, 
  author={Chen, Luc and He, Yinan and Nguyen, Alayna},
  year={2024},
  month={12},
  day={30},
  eprint={20241230074419},
  archivePrefix={GitData Archive},
  primaryClass={Computer Science (Computation and Language)},
  url={https://archive.gd.edu.kg/abs/20241230074419/}, 
}</pre>
              <button onclick="copyCode()" class="fulltext" style="position: absolute; top: 15px; right: 15px;">Copy</button>
            </div>
            <p>
              Chen, L.  et al. "Data Efficient Dense Cross-Lingual Information Retrieval." <i>GitData Archive</i>, vol. 24, Dec. 2024, <a href="https://archive.gd.edu.kg/abs/20241230074419/">https://archive.gd.edu.kg/abs/20241230074419/</a>
            </p>
            
          </div>
          <div class="abstract">
            <span>
              <b>Abstract: </b>
            </span>
            <br>
            <p>Cross-Lingual Information Retrieval (CIR) remains challenging due to limited annotated data and linguistic diversity, especially for low-resource languages. While dense retrieval models have significantly advanced retrieval performance, their reliance on large-scale training datasets hampers their effectiveness in multilingual settings. In this work, we propose two complementary strategies to improve data efficiency and robustness in CIR model fine- tuning. First, we introduce a paraphrase-based query augmentation pipeline leveraging large language models (LLMs) to enrich scarce training data, thereby promoting more robust and language-agnostic representations. Second, we present a weighted InfoNCE loss that emphasizes underrepresented languages, ensuring balanced optimization across heterogeneous linguistic inputs. Experiments on cross-lingual benchmark datasets demonstrate that our combined approaches yield substantial gains in retrieval quality, outperforming standard training protocols on small and imbalanced datasets. These results underscore the potential of targeted data augmentation and reweighted objectives to build more inclusive and effective CIR systems, even under resource constraints.</p>
          </div>
          <div class="preview">
            <span>
              <b>Preview: </b>
            </span>
            <br>
            <a href="https://pdfviewer.gd.edu.kg/web/viewer?file=https://archive.gd.edu.kg/pdf/20241230074419/v1.pdf" target="_blank">Preview in a new window</a>
            </br>
            <iframe src="https://pdfviewer.gd.edu.kg/web/viewer?file=https://archive.gd.edu.kg/pdf/20241230074419/v1.pdf" loading="lazy" width="100%" height="500" frameBorder="0" alt="pdf preview"></iframe>
          </div>
          <div class="license">
            <span>
              <b>License: </b>
            </span>
            <br>
            <p xmlns:cc="http://creativecommons.org/ns#" >This work is licensed under <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY 4.0</a>.</p>
          </div>
          <div id="gdArchiveLab">
          </div>
          <br>
          <br>
        </div>
        <div class="article-info col side-block">
          <div class="block-head">
            <h2>Information</h2>
          </div>
          <div class="side-block_container">
            <div id="versionInfo" class="side-block_section">
              <span class="head">Version</span>
              <ul>
                <li>
                  <span>Version 1</span>
                </li>
              </ul>
            </div>
            <div class="side-block_section">
              <span class="head">Submission Date (yyyy/mm/dd)</span>
              <p>2024/12/30</p>
            </div>
            <div class="side-block_section">
              <span class="head">Author</span>
              <ul>
                <li key=0>
                        <span>Luc Chen</span>
                      </li><li key=1>
                        <span>Yinan He</span>
                      </li><li key=2>
                        <span>Alayna Nguyen</span>
                      </li>
              </ul>
            </div>
            <div class="side-block_section">
              <span class="head">Subject</span>
              <ul>
                <li>Computer Science (Computation and Language)</li>
              </ul>
            </div>
            <div class="side-block_section">
              <span class="head">Keywords</span>
              <ul>
                <li>
                  
                      LLMs
                      <br>
                  
                      Cross-Lingual Information Retrieval (CIR)
                      <br>
                  
                      Translation
                      
                  
                </li>
              </ul>
            </div>
            
            
            <div class="side-block_section">
              <span class="head">Download</span>
              <ul>
                <li>
                  <a id="downloadLink" onclick="downloadFile('https://archive.gd.edu.kg/pdf/20241230074419/v1.pdf')">
                    <button id="downloadButton" class="fulltext">
                      <span class="ext">PDF</span>
                      <span class="label">Full Text</span>
                    </button>
                  </a>
                  <img src="https://archive.gd.edu.kg/assets/loading.gif" alt="loading" width="20px" style="visibility: hidden;" id="downloadLoading" />
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
      <div class="footer">
        <div id="footerMenu" class="container"></div>
        <div class="col-5 np"> &copy; <script type="text/JavaScript"> document.write(new Date().getFullYear()) </script> GitData Archive </div>
      </div>
    </div>
  </body>
</html>